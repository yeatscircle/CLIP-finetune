{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import Packages",
   "id": "317b03bd2d23704"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T10:21:44.381978Z",
     "start_time": "2024-04-29T10:21:42.130666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch.nn as nn\n",
    "import math\n",
    "# 用于显示进度条\n",
    "from tqdm import tqdm\n",
    "# 绘制评估曲线\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ],
   "id": "bde74edf0cbea8ba",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "15dfabaa2b9066f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T10:21:46.625096Z",
     "start_time": "2024-04-29T10:21:46.603180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = {\n",
    "    'seed': 6666,\n",
    "    'dataset_dir': \"C:\\\\dats_tec\\\\archive\",\n",
    "    'n_epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'early_stop': 3,\n",
    "    'clip_flag': True,\n",
    "    'save_path': './models/model.ckpt',\n",
    "    'resnet_save_path': './models/resnet_model.ckpt',\n",
    "    'num_workers': 12\n",
    "}\n"
   ],
   "id": "a8bd4df3b98f3f2b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Some Utility Function",
   "id": "9d0383bb35cbef16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T10:21:50.466929Z",
     "start_time": "2024-04-29T10:21:50.450484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 设置全局的随机种子\n",
    "def all_seed(seed=6666):\n",
    "    \"\"\"\n",
    "    设置随机种子\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # CPU\n",
    "    torch.manual_seed(seed)\n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        # python 全局\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    print(f'Set env random_seed = {seed}')\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n"
   ],
   "id": "55cd314fc58de4aa",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model-Clip",
   "id": "3bf6ade490f0665b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T10:22:15.725192Z",
     "start_time": "2024-04-29T10:22:15.721192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CLIPClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=100):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "        self.classifier = nn.Linear(self.clip.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids):\n",
    "        # clip有两个部分,一个NLP部分一个是Vision部分\n",
    "        vision_outputs = self.clip.vision_model(pixel_values=pixel_values)\n",
    "        text_outputs = self.clip.text_model(input_ids=input_ids)\n",
    "\n",
    "        # 这是Vision_modelt与text_model的结果在经过最终的池化过程后得到的固定长度的向量,一般形式为(batch_size,hidden_layers)\n",
    "        image_features = vision_outputs.pooler_output\n",
    "        text_features = text_outputs.pooler_output\n",
    "\n",
    "        # 文本特征融合\n",
    "        combined_features = torch.cat((image_features, text_features), dim=-1)\n",
    "        \n",
    "        # 进行了一个分类操作\n",
    "        logits = self.classifier(combined_features)\n",
    "        return logits\n"
   ],
   "id": "2fde6a86e1c60e77",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transformer",
   "id": "a8f88bf30deec736"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T07:12:15.094666Z",
     "start_time": "2024-04-29T07:12:15.091666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 一般情况下，我们不会在验证集和测试集上做数据扩增\n",
    "# 我们只需要将图片裁剪成同样的大小并装换成Tensor就行\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 当然，我们也可以再测试集中对数据进行扩增（对同样本的不同装换）\n",
    "#  - 用训练数据的装化方法（train_tfm）去对测试集数据进行转化，产出扩增样本\n",
    "#  - 对同个照片的不同样本分别进行预测\n",
    "#  - 最后可以用soft vote / hard vote 等集成方法输出最后的预测\n",
    "train_tfm = transforms.Compose([\n",
    "    # 图片裁剪 (height = width = 224)\n",
    "    transforms.Resize((224, 224)),\n",
    "    # TODO:在这部分还可以增加一些图片处理的操作\n",
    "    transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET),\n",
    "    # ToTensor() 放在所有处理的最后\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def text_transformer(text):\n",
    "    # 使用 CLIP 的处理器将文本转换为模型需要的格式\n",
    "    # 这会自动处理标记化、数字化和张量化\n",
    "    # return_tensors=\"pt\" 告诉处理器返回 PyTorch 张量\n",
    "    processed = processor(text=text, return_tensors=\"pt\")\n",
    "    # 通常只需要 input_ids，这是输入到模型的实际文本张量\n",
    "    return processed.input_ids.squeeze()  # 移除不必要的批处理维度\n",
    "\n",
    "\n",
    "def labels_to_text(label):\n",
    "    return 'A photo of ' + label"
   ],
   "id": "99342da5729f6a2d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "df3bd54f4f3ac878"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T07:13:20.761485Z",
     "start_time": "2024-04-29T07:13:20.751475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class sportsDataset(Dataset):\n",
    "    def __init__(self, path, transformer=test_tfm, text_transformer=text_transformer, label_to_text=labels_to_text):\n",
    "        self.transformer = transformer\n",
    "        self.text_transformer = text_transformer\n",
    "        self.label_to_text = label_to_text\n",
    "        self.labels = []\n",
    "        self.images = []\n",
    "        for dirpath, dirnames, filenames in os.walk(path):\n",
    "            ''' \n",
    "            dirpath 是当前正在遍历的文件夹的路径\n",
    "            dirnames 是当前文件夹中所有子文件夹的名字列表\n",
    "            filenames 是当前文件夹中所有文件的名字列表\n",
    "            '''\n",
    "            for dirname in dirnames:\n",
    "                dir = os.path.join(dirpath, dirname)\n",
    "                for file in os.listdir(dir):\n",
    "                    self.labels.append(dirname)\n",
    "                    self.images.append(os.path.join(dir, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx]).convert('RGB')\n",
    "        image = self.transformer(image)\n",
    "        text = self.text_transformer(labels_to_text(self.labels[idx]))\n",
    "        return text, image\n"
   ],
   "id": "f44c8e55d7930da",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train",
   "id": "9e01b3dcbc8be1ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T07:45:39.864611Z",
     "start_time": "2024-04-29T07:45:39.840766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def trainer(train_loader, valid_loader, model, config, device, rest_net_flag=False):\n",
    "    # 对于分类任务, 我们常用cross-entropy评估模型表现.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # 初始化优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5) \n",
    "    # 模型存储位置\n",
    "    save_path = config['save_path'] if rest_net_flag else config['resnet_save_path']\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models')\n",
    "\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        loss_record = []\n",
    "        train_accs = []\n",
    "        train_pbar = tqdm(train_loader, position=0, leave=True)\n",
    "\n",
    "        for x, y in train_pbar:\n",
    "            scheduler.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "\n",
    "            scheduler.step()\n",
    "            step += 1\n",
    "            acc = (pred.argmax(dim=-1) == y.to(device)).float().mean()\n",
    "            l_ = loss.detach().item()\n",
    "            loss_record.append(l_)\n",
    "            train_accs.append(acc.detach().item())\n",
    "            train_pbar.set_description(f'Epoch [{epoch + 1}/{n_epochs}]')\n",
    "            train_pbar.set_postfix({'loss': f'{l_:.5f}', 'acc': f'{acc:.5f}'})\n",
    "\n",
    "        mean_train_acc = sum(train_accs) / len(train_accs)\n",
    "        mean_train_loss = sum(loss_record) / len(loss_record)\n",
    "        writer.add_scalar('Loss/train', mean_train_loss, step)\n",
    "        writer.add_scalar('ACC/train', mean_train_acc, step)\n",
    "        model.eval()  # 设置模型为评估模式\n",
    "        loss_record = []\n",
    "        test_accs = []\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "                acc = (pred.argmax(dim=-1) == y.to(device)).float().mean()\n",
    "\n",
    "            loss_record.append(loss.item())\n",
    "            test_accs.append(acc.detach().item())\n",
    "\n",
    "        mean_valid_acc = sum(test_accs) / len(test_accs)\n",
    "        mean_valid_loss = sum(loss_record) / len(loss_record)\n",
    "        print(\n",
    "            f'Epoch [{epoch + 1}/{n_epochs}]: Train loss: {mean_train_loss:.4f},acc: {mean_train_acc:.4f} Valid loss: {mean_valid_loss:.4f},acc: {mean_valid_acc:.4f} ')\n",
    "        writer.add_scalar('Loss/valid', mean_valid_loss, step)\n",
    "        writer.add_scalar('ACC/valid', mean_valid_acc, step)\n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            torch.save(model.state_dict(), save_path)  # 保存最优模型\n",
    "            print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            return\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 设计随机种子\n",
    "    all_seed(config['seed'])\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 创建数据集\n",
    "    train_dataset = sportsDataset(os.path.join(config['dataset_dir'], 'train'),\n",
    "                                  transformer=train_tfm)\n",
    "    valid_dataset = sportsDataset(os.path.join(config['dataset_dir'], 'valid'),\n",
    "                                  transformer=test_tfm)\n",
    "    # 装载数据\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True,\n",
    "                              num_workers=config['num_workers'], pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True,\n",
    "                              num_workers=config['num_workers'], pin_memory=True)\n",
    "\n",
    "    my_model = CLIPClassifier(model).to(device)\n",
    "\n",
    "    trainer(train_loader, valid_loader, my_model, config, device)"
   ],
   "id": "5c2917ac3ce35f9f",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 运行",
   "id": "9905131d0fc56631"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-29T07:45:41.841567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "1fcac4db9a100018",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set env random_seed = 6666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/211 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "53864c21d22620c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
